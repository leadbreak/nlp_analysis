{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "HIDDEN_DIM = 256\n",
    "NUM_HEAD = 8 \n",
    "INNER_DIM = 512\n",
    "\n",
    "PAD_IDX = 0\n",
    "EOS_IDX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddir = './'\n",
    "\n",
    "src_train_path = os.path.join(ddir,'src_train.pkl')\n",
    "src_valid_path = os.path.join(ddir,'src_valid.pkl')\n",
    "trg_train_path = os.path.join(ddir,'trg_train.pkl')\n",
    "trg_valid_path = os.path.join(ddir,'trg_valid.pkl')\n",
    "\n",
    "src_train_path2 = os.path.join(ddir,'src_train2.pkl')\n",
    "src_valid_path2 = os.path.join(ddir,'src_valid2.pkl')\n",
    "trg_train_path2 = os.path.join(ddir,'trg_train2.pkl')\n",
    "trg_valid_path2 = os.path.join(ddir,'trg_valid2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = joblib.load(src_train_path)\n",
    "src_valid = joblib.load(src_valid_path)\n",
    "trg_train = joblib.load(trg_train_path)\n",
    "trg_valid = joblib.load(trg_valid_path)\n",
    "\n",
    "src_train2 = joblib.load(src_train_path2)\n",
    "src_valid2 = joblib.load(src_valid_path2)\n",
    "trg_train2 = joblib.load(trg_train_path2)\n",
    "trg_valid2 = joblib.load(trg_valid_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entertain',\n",
       " 'society',\n",
       " 'international',\n",
       " 'economy',\n",
       " 'culture',\n",
       " 'politics',\n",
       " 'it',\n",
       " 'sport']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(set(trg_train))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entertain': 0,\n",
       " 'society': 1,\n",
       " 'international': 2,\n",
       " 'economy': 3,\n",
       " 'culture': 4,\n",
       " 'politics': 5,\n",
       " 'it': 6,\n",
       " 'sport': 7}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict = {}\n",
    "for i in range(len(labels)) :\n",
    "    labels_dict[labels[i]] = i\n",
    "labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiLabelEncoder(labels_dict, target) :\n",
    "    tmp = np.zeros((len(target), len(labels_dict)))\n",
    "    for t in range(len(target)) :\n",
    "        tmp[t][labels_dict[target[t]]] = 1\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['politics', 'politics', 'politics', 'society', 'economy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_valid2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = multiLabelEncoder(labels_dict, trg_valid2)\n",
    "test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_train = multiLabelEncoder(labels_dict, trg_train)\n",
    "trg_valid = multiLabelEncoder(labels_dict, trg_valid)\n",
    "trg_train2 = multiLabelEncoder(labels_dict, trg_train2)\n",
    "trg_valid2 = multiLabelEncoder(labels_dict, trg_valid2)\n",
    "trg_valid2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 15*100*8\n",
    "SEQ_LEN = 60*2\n",
    "\n",
    "VOCAB_SIZE2 = 1108*8\n",
    "SEQ_LEN2 = 4674*2\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, src_data, trg_data):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_data = src_data\n",
    "        self.trg_data = trg_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "        \n",
    "    def __getitem__ (self, idx):\n",
    "        src = self.src_data[idx]\n",
    "        trg = self.trg_data[idx]\n",
    "\n",
    "        return torch.Tensor(src).long(), torch.Tensor(trg).long()\n",
    "\n",
    "train_dataset = TrainDataset(src_train, trg_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle= True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset(Dataset):\n",
    "    def __init__(self, src_data, trg_data):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_data = src_data\n",
    "        self.trg_data = trg_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "        \n",
    "    def __getitem__ (self, idx):\n",
    "        src = self.src_data[idx]\n",
    "        trg = self.trg_data[idx]\n",
    "\n",
    "        return torch.Tensor(src).long(), torch.Tensor(trg).long()\n",
    "\n",
    "valid_dataset = ValidDataset(src_valid, trg_valid)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle= False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__ (self, hidden_dim, inner_dim):\n",
    "        super().__init__()\n",
    " \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.inner_dim = inner_dim \n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, inner_dim)\n",
    "        self.fc2 = nn.Linear(inner_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "   \n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        output = self.fc1(output)\n",
    "        output2 = self.relu(output)\n",
    "        output2 = self.dropout(output)\n",
    "        output3 = self.fc2(output2)\n",
    "\n",
    "        return output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def makeMask(tensor, option: str) -> torch.Tensor:\n",
    "  \n",
    "    if option == 'padding':\n",
    "        tmp = torch.full_like(tensor, fill_value=PAD_IDX).to(device)\n",
    "       \n",
    "        mask = (tensor != tmp).float()\n",
    "        \n",
    "        mask = rearrange(mask, 'bs seq_len -> bs 1 1 seq_len ')\n",
    "\n",
    "    elif option == 'lookahead':\n",
    "\n",
    "        padding_mask = makeMask(tensor, 'padding')\n",
    "        padding_mask = repeat(\n",
    "            padding_mask, 'bs 1 1 k_len -> bs 1 new k_len', new=padding_mask.shape[3])\n",
    "        \n",
    "        mask = torch.ones_like(padding_mask)\n",
    "        mask = torch.tril(mask)\n",
    "\n",
    "        mask = mask * padding_mask\n",
    "        \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiheadattention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_head: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding_dim, d_model, 512 in paper\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # 8 in paper\n",
    "        self.num_head = num_head\n",
    "        # head_dim, d_key, d_query, d_value, 64 in paper (= 512 / 8)\n",
    "        self.head_dim = hidden_dim // num_head\n",
    "        self.scale = torch.sqrt(torch.FloatTensor()).to(device)\n",
    "\n",
    "        self.fcQ = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fcK = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fcV = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fcOut = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, srcQ, srcK, srcV, mask=None):\n",
    "\n",
    "        ##### SCALED DOT PRODUCT ATTENTION ######\n",
    "\n",
    "        Q = self.fcQ(srcQ)\n",
    "        K = self.fcK(srcK)\n",
    "        V = self.fcV(srcV)\n",
    "\n",
    "        Q = rearrange(\n",
    "            Q, 'bs seq_len (num_head head_dim) -> bs num_head seq_len head_dim', num_head=self.num_head)\n",
    "        K_T = rearrange(\n",
    "            K, 'bs seq_len (num_head head_dim) -> bs num_head head_dim seq_len', num_head=self.num_head)\n",
    "        V = rearrange(\n",
    "            V, 'bs seq_len (num_head head_dim) -> bs num_head seq_len head_dim', num_head=self.num_head)\n",
    "        \n",
    "        attention_energy = torch.matmul(Q, K_T)\n",
    "\n",
    "        if mask is not None :\n",
    " \n",
    "            attention_energy = torch.masked_fill(attention_energy, (mask == 0), -1e+4)\n",
    "            \n",
    "        attention_energy = torch.softmax(attention_energy, dim = -1)\n",
    "\n",
    "        result = torch.matmul(self.dropout(attention_energy),V)\n",
    "\n",
    "        ##### END OF SCALED DOT PRODUCT ATTENTION ######\n",
    "\n",
    "        # CONCAT\n",
    "        result = rearrange(result, 'bs num_head seq_len head_dim -> bs seq_len (num_head head_dim)')\n",
    "\n",
    "        result = self.fcOut(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_head, inner_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_head = num_head\n",
    "        self.inner_dim = inner_dim\n",
    "        \n",
    "        self.multiheadattention = Multiheadattention(hidden_dim, num_head)\n",
    "        self.ffn = FFN(hidden_dim, inner_dim)\n",
    "        self.layerNorm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layerNorm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "\n",
    "\n",
    "    def forward(self, input, mask = None):\n",
    "\n",
    "        output = self.multiheadattention(srcQ= input, srcK = input, srcV = input, mask = mask)\n",
    "        output = self.dropout1(output)\n",
    "        output = input + output\n",
    "        output = self.layerNorm1(output)\n",
    "\n",
    "        output_ = self.ffn(output)\n",
    "        output_ = self.dropout2(output_)\n",
    "        output = output + output_\n",
    "        output = self.layerNorm2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__ (self, N, hidden_dim, num_head, inner_dim,max_length=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # N : number of encoder layer repeated \n",
    "        self.N = N\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_head = num_head\n",
    "        self.inner_dim = inner_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=VOCAB_SIZE, embedding_dim=hidden_dim, padding_idx=-1)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "        self.enc_layers = nn.ModuleList([EncoderLayer(hidden_dim, num_head, inner_dim) for _ in range(N)])\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        batch_size = input.shape[0]\n",
    "        seq_len = input.shape[1]\n",
    "\n",
    "\n",
    "        mask = makeMask(input, option='padding')\n",
    "\n",
    "        pos = torch.arange(0, seq_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "\n",
    "        output = self.dropout(self.embedding(input) + self.pos_embedding(pos))\n",
    "\n",
    "        # Dropout\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # N encoder layer\n",
    "        for layer in self.enc_layers:\n",
    "            output = layer(output, mask)\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N = 2, hidden_dim = 256, num_head = 8, inner_dim = 512):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(N, hidden_dim, num_head, inner_dim)\n",
    "        self.mlp = nn.Sequential(nn.Linear(256, 64),\n",
    "                                 nn.Linear(64,16),\n",
    "                                 nn.GELU(),\n",
    "                                 nn.Linear(16,8)\n",
    "        )\n",
    "\n",
    "    def forward(self, enc_src):\n",
    "\n",
    "        enc_output = self.encoder(enc_src)\n",
    "        pred = self.mlp(enc_output)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(12000, 256, padding_idx=11999)\n",
       "    (pos_embedding): Embedding(1000, 256)\n",
       "    (enc_layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (multiheadattention): Multiheadattention(\n",
       "          (fcQ): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fcK): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fcV): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fcOut): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ffn): FFN(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layerNorm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layerNorm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (multiheadattention): Multiheadattention(\n",
       "          (fcQ): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fcK): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fcV): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fcOut): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ffn): FFN(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layerNorm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layerNorm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (2): GELU()\n",
       "    (3): Linear(in_features=16, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(N, HIDDEN_DIM, NUM_HEAD, INNER_DIM).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4, weight_decay = 0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "\n",
    "    for step, (src, trg) in bar:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        pred = model(enc_src=src)\n",
    "        \n",
    "        loss = criterion(pred, trg)\n",
    "\n",
    "        loss.backward()\n",
    "    \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  \n",
    "     \n",
    "        optimizer.step()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # change learning rate by Scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_accuracy = np.mean(pred.detach().cpu().numpy() == trg.detach().cpu().numpy())\n",
    "\n",
    "        accuracy += running_accuracy\n",
    "\n",
    "        dataset_size += batch_size\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        bar.set_postfix(\n",
    "            Epoch=epoch, Train_Loss=epoch_loss, LR=optimizer.param_groups[0][\"lr\"], accuracy=accuracy / np.float(\n",
    "                step+1)\n",
    "        )\n",
    "\n",
    "    accuracy /= len(dataloader)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "\n",
    "    for step, (src, trg) in bar:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        pred = model(enc_src = src)\n",
    "        loss = criterion(pred, trg)\n",
    "\n",
    "        running_loss += loss.item() * batch_size\n",
    "        dataset_size += batch_size\n",
    "\n",
    "     \n",
    "        val_loss = running_loss / dataset_size\n",
    "        running_accuracy = np.mean(pred.view(-1).detach().cpu().numpy() == trg.view(-1).detach().cpu().numpy())\n",
    "        \n",
    "        accuracy += running_accuracy\n",
    "\n",
    "        bar.set_postfix(\n",
    "            Epoch=epoch, Valid_Loss=val_loss, LR=optimizer.param_groups[0][\"lr\"], accuracy = accuracy / np.float(step + 1)\n",
    "        )\n",
    "\n",
    "    accuracy /= len(dataloader)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    metric_prefix=\"\",\n",
    "    file_prefix=\"\",\n",
    "    early_stopping=True,\n",
    "    early_stopping_step=10,\n",
    "):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU:{}\\n\".format(torch.cuda.get_device_name()))\n",
    "\n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        gc.collect()\n",
    "\n",
    "        train_epoch_loss, train_accuracy = train_one_epoch(\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            dataloader= train_dataloader,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "        val_loss, val_accuracy = valid_one_epoch(\n",
    "            model, valid_dataloader, device=device, epoch=epoch\n",
    "        )\n",
    "\n",
    "        history[f\"{metric_prefix}Train Loss\"].append(train_epoch_loss)\n",
    "        history[f\"{metric_prefix}Train Accuracy\"].append(train_accuracy)\n",
    "        history[f\"{metric_prefix}Valid Loss\"].append(val_loss)\n",
    "        history[f\"{metric_prefix}Valid Accuracy\"].append(val_accuracy)\n",
    "\n",
    "\n",
    "        print(f\"Valid Loss : {val_loss}\")\n",
    "\n",
    "        if val_loss <= best_loss:\n",
    "            early_stop_counter = 0\n",
    "\n",
    "            print(\n",
    "                f\"Validation Loss improved( {best_loss} ---> {val_loss}  )\"\n",
    "            )\n",
    "\n",
    "            # Update Best Loss\n",
    "            best_loss = val_loss\n",
    "            \n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            PATH = \"{}epoch{:.0f}_Loss{:.4f}.bin\".format(file_prefix, epoch, best_loss)\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            torch.save(model.state_dict(), f\"{file_prefix}best_{epoch}epoch.bin\")\n",
    "\n",
    "            print(f\"Model Saved\")\n",
    "\n",
    "        elif early_stopping:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter > early_stopping_step:\n",
    "                break\n",
    "        \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print(\n",
    "        \"Training complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 3600,\n",
    "            (time_elapsed % 3600) // 60,\n",
    "            (time_elapsed % 3600) % 60,\n",
    "        )\n",
    "    )\n",
    "    print(\"Best Loss: {:.4f}\".format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU:NVIDIA A100-PCIE-40GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/tmp/ipykernel_726001/1528314979.py:36: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  running_accuracy = np.mean(pred.detach().cpu().numpy() == trg.detach().cpu().numpy())\n",
      "/tmp/ipykernel_726001/1528314979.py:44: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Epoch=epoch, Train_Loss=epoch_loss, LR=optimizer.param_groups[0][\"lr\"], accuracy=accuracy / np.float(\n",
      "100%|██████████| 40/40 [00:00<00:00, 58.74it/s, Epoch=1, LR=6.89e-5, Train_Loss=3.71, accuracy=0]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_726001/2078626933.py:25: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  running_accuracy = np.mean(pred.view(-1).detach().cpu().numpy() == trg.view(-1).detach().cpu().numpy())\n",
      "/tmp/ipykernel_726001/2078626933.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  Epoch=epoch, Valid_Loss=val_loss, LR=optimizer.param_groups[0][\"lr\"], accuracy = accuracy / np.float(step + 1)\n",
      "100%|██████████| 10/10 [00:00<00:00, 155.57it/s, Epoch=1, LR=6.89e-5, Valid_Loss=2.64, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 2.6440712555198913\n",
      "Validation Loss improved( inf ---> 2.6440712555198913  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 58.22it/s, Epoch=2, LR=1.86e-5, Train_Loss=2.33, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 201.53it/s, Epoch=2, LR=1.86e-5, Valid_Loss=1.92, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 1.9240868767355657\n",
      "Validation Loss improved( 2.6440712555198913 ---> 1.9240868767355657  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 69.18it/s, Epoch=3, LR=1.86e-5, Train_Loss=1.96, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 204.40it/s, Epoch=3, LR=1.86e-5, Valid_Loss=1.76, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 1.7618302539655357\n",
      "Validation Loss improved( 1.9240868767355657 ---> 1.7618302539655357  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 67.69it/s, Epoch=4, LR=6.89e-5, Train_Loss=1.7, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 201.34it/s, Epoch=4, LR=6.89e-5, Valid_Loss=1.33, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 1.3331764982004835\n",
      "Validation Loss improved( 1.7618302539655357 ---> 1.3331764982004835  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 68.25it/s, Epoch=5, LR=0.0001, Train_Loss=1.08, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 194.90it/s, Epoch=5, LR=0.0001, Valid_Loss=0.686, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.685522800418222\n",
      "Validation Loss improved( 1.3331764982004835 ---> 0.685522800418222  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 71.34it/s, Epoch=6, LR=6.89e-5, Train_Loss=0.554, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 226.24it/s, Epoch=6, LR=6.89e-5, Valid_Loss=0.425, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.4250426349366546\n",
      "Validation Loss improved( 0.685522800418222 ---> 0.4250426349366546  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 65.16it/s, Epoch=7, LR=1.86e-5, Train_Loss=0.412, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 163.90it/s, Epoch=7, LR=1.86e-5, Valid_Loss=0.392, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.39159469277995407\n",
      "Validation Loss improved( 0.4250426349366546 ---> 0.39159469277995407  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 50.38it/s, Epoch=8, LR=1.86e-5, Train_Loss=0.395, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 140.83it/s, Epoch=8, LR=1.86e-5, Valid_Loss=0.386, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3863049141920296\n",
      "Validation Loss improved( 0.39159469277995407 ---> 0.3863049141920296  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 58.91it/s, Epoch=9, LR=6.89e-5, Train_Loss=0.387, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 184.60it/s, Epoch=9, LR=6.89e-5, Valid_Loss=0.375, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3745458615813286\n",
      "Validation Loss improved( 0.3863049141920296 ---> 0.3745458615813286  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 61.76it/s, Epoch=10, LR=0.0001, Train_Loss=0.371, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 207.89it/s, Epoch=10, LR=0.0001, Valid_Loss=0.362, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3615802325260867\n",
      "Validation Loss improved( 0.3745458615813286 ---> 0.3615802325260867  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 68.43it/s, Epoch=11, LR=6.89e-5, Train_Loss=0.359, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 202.16it/s, Epoch=11, LR=6.89e-5, Valid_Loss=0.354, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3544577400016177\n",
      "Validation Loss improved( 0.3615802325260867 ---> 0.3544577400016177  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 70.25it/s, Epoch=12, LR=1.86e-5, Train_Loss=0.353, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 189.79it/s, Epoch=12, LR=1.86e-5, Valid_Loss=0.352, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.35239705860994425\n",
      "Validation Loss improved( 0.3544577400016177 ---> 0.35239705860994425  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 69.28it/s, Epoch=13, LR=1.86e-5, Train_Loss=0.351, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 222.10it/s, Epoch=13, LR=1.86e-5, Valid_Loss=0.352, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.35163886133272937\n",
      "Validation Loss improved( 0.35239705860994425 ---> 0.35163886133272937  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 66.99it/s, Epoch=14, LR=6.89e-5, Train_Loss=0.349, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 168.56it/s, Epoch=14, LR=6.89e-5, Valid_Loss=0.35, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3496918951629833\n",
      "Validation Loss improved( 0.35163886133272937 ---> 0.3496918951629833  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 72.10it/s, Epoch=15, LR=0.0001, Train_Loss=0.345, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 205.26it/s, Epoch=15, LR=0.0001, Valid_Loss=0.347, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.34679678993619933\n",
      "Validation Loss improved( 0.3496918951629833 ---> 0.34679678993619933  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 66.27it/s, Epoch=16, LR=6.89e-5, Train_Loss=0.342, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 211.41it/s, Epoch=16, LR=6.89e-5, Valid_Loss=0.344, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3441055775827663\n",
      "Validation Loss improved( 0.34679678993619933 ---> 0.3441055775827663  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 70.77it/s, Epoch=17, LR=1.86e-5, Train_Loss=0.338, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 170.02it/s, Epoch=17, LR=1.86e-5, Valid_Loss=0.342, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3417198788967861\n",
      "Validation Loss improved( 0.3441055775827663 ---> 0.3417198788967861  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 68.97it/s, Epoch=18, LR=1.86e-5, Train_Loss=0.337, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 229.75it/s, Epoch=18, LR=1.86e-5, Valid_Loss=0.341, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3411487625662688\n",
      "Validation Loss improved( 0.3417198788967861 ---> 0.3411487625662688  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 68.57it/s, Epoch=19, LR=6.89e-5, Train_Loss=0.337, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 181.81it/s, Epoch=19, LR=6.89e-5, Valid_Loss=0.34, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3395765144733866\n",
      "Validation Loss improved( 0.3411487625662688 ---> 0.3395765144733866  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 73.09it/s, Epoch=20, LR=0.0001, Train_Loss=0.335, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 226.05it/s, Epoch=20, LR=0.0001, Valid_Loss=0.337, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3366292563213664\n",
      "Validation Loss improved( 0.3395765144733866 ---> 0.3366292563213664  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 68.27it/s, Epoch=21, LR=6.89e-5, Train_Loss=0.33, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 238.45it/s, Epoch=21, LR=6.89e-5, Valid_Loss=0.338, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.33840304651078146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 67.12it/s, Epoch=22, LR=1.86e-5, Train_Loss=0.324, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 204.41it/s, Epoch=22, LR=1.86e-5, Valid_Loss=0.333, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3327225450497524\n",
      "Validation Loss improved( 0.3366292563213664 ---> 0.3327225450497524  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 72.41it/s, Epoch=23, LR=1.86e-5, Train_Loss=0.322, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 223.60it/s, Epoch=23, LR=1.86e-5, Valid_Loss=0.333, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.33260982621247603\n",
      "Validation Loss improved( 0.3327225450497524 ---> 0.33260982621247603  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 69.30it/s, Epoch=24, LR=6.89e-5, Train_Loss=0.32, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 208.19it/s, Epoch=24, LR=6.89e-5, Valid_Loss=0.33, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3303360198713412\n",
      "Validation Loss improved( 0.33260982621247603 ---> 0.3303360198713412  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 74.29it/s, Epoch=25, LR=0.0001, Train_Loss=0.318, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 231.38it/s, Epoch=25, LR=0.0001, Valid_Loss=0.328, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3282006502531137\n",
      "Validation Loss improved( 0.3303360198713412 ---> 0.3282006502531137  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 69.51it/s, Epoch=26, LR=6.89e-5, Train_Loss=0.311, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 239.40it/s, Epoch=26, LR=6.89e-5, Valid_Loss=0.325, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3250592278826768\n",
      "Validation Loss improved( 0.3282006502531137 ---> 0.3250592278826768  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 70.30it/s, Epoch=27, LR=1.86e-5, Train_Loss=0.304, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 183.80it/s, Epoch=27, LR=1.86e-5, Valid_Loss=0.323, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.32303788593620253\n",
      "Validation Loss improved( 0.3250592278826768 ---> 0.32303788593620253  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 54.58it/s, Epoch=28, LR=1.86e-5, Train_Loss=0.3, accuracy=0]  \n",
      "100%|██████████| 10/10 [00:00<00:00, 165.22it/s, Epoch=28, LR=1.86e-5, Valid_Loss=0.323, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3225363371478524\n",
      "Validation Loss improved( 0.32303788593620253 ---> 0.3225363371478524  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 59.91it/s, Epoch=29, LR=6.89e-5, Train_Loss=0.298, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 209.64it/s, Epoch=29, LR=6.89e-5, Valid_Loss=0.324, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.32387557948470874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 60.76it/s, Epoch=30, LR=0.0001, Train_Loss=0.299, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 191.99it/s, Epoch=30, LR=0.0001, Valid_Loss=0.321, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3212295083483313\n",
      "Validation Loss improved( 0.3225363371478524 ---> 0.3212295083483313  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 70.83it/s, Epoch=31, LR=6.89e-5, Train_Loss=0.292, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 228.10it/s, Epoch=31, LR=6.89e-5, Valid_Loss=0.319, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3185038655806499\n",
      "Validation Loss improved( 0.3212295083483313 ---> 0.3185038655806499  )\n",
      "Model Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 68.58it/s, Epoch=32, LR=1.86e-5, Train_Loss=0.28, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 217.09it/s, Epoch=32, LR=1.86e-5, Valid_Loss=0.321, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.32071771249649633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 74.30it/s, Epoch=33, LR=1.86e-5, Train_Loss=0.279, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 196.44it/s, Epoch=33, LR=1.86e-5, Valid_Loss=0.32, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.32015845749028926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 65.79it/s, Epoch=34, LR=6.89e-5, Train_Loss=0.277, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 194.64it/s, Epoch=34, LR=6.89e-5, Valid_Loss=0.322, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3222711196370945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 60.93it/s, Epoch=35, LR=0.0001, Train_Loss=0.277, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 184.69it/s, Epoch=35, LR=0.0001, Valid_Loss=0.324, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.323979553523337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 69.61it/s, Epoch=36, LR=6.89e-5, Train_Loss=0.268, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 218.92it/s, Epoch=36, LR=6.89e-5, Valid_Loss=0.328, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3277472534756752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 64.05it/s, Epoch=37, LR=1.86e-5, Train_Loss=0.263, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 170.74it/s, Epoch=37, LR=1.86e-5, Valid_Loss=0.327, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.32672906700213245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 67.54it/s, Epoch=38, LR=1.86e-5, Train_Loss=0.255, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 203.92it/s, Epoch=38, LR=1.86e-5, Valid_Loss=0.325, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.32507601133577385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 65.96it/s, Epoch=39, LR=6.89e-5, Train_Loss=0.257, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 227.76it/s, Epoch=39, LR=6.89e-5, Valid_Loss=0.331, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.33080442354177975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 69.55it/s, Epoch=40, LR=0.0001, Train_Loss=0.258, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 208.87it/s, Epoch=40, LR=0.0001, Valid_Loss=0.326, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3262888005205021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 74.82it/s, Epoch=41, LR=6.89e-5, Train_Loss=0.25, accuracy=0] \n",
      "100%|██████████| 10/10 [00:00<00:00, 220.61it/s, Epoch=41, LR=6.89e-5, Valid_Loss=0.328, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3280114288542681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 66.00it/s, Epoch=42, LR=1.86e-5, Train_Loss=0.246, accuracy=0]\n",
      "100%|██████████| 10/10 [00:00<00:00, 198.22it/s, Epoch=42, LR=1.86e-5, Valid_Loss=0.328, accuracy=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss : 0.3282806338018672\n",
      "Training complete in 0h 0m 40s\n",
      "Best Loss: 0.3185\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Transformer(\n",
       "   (encoder): Encoder(\n",
       "     (embedding): Embedding(12000, 256, padding_idx=11999)\n",
       "     (pos_embedding): Embedding(1000, 256)\n",
       "     (enc_layers): ModuleList(\n",
       "       (0): EncoderLayer(\n",
       "         (multiheadattention): Multiheadattention(\n",
       "           (fcQ): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (fcK): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (fcV): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (fcOut): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ffn): FFN(\n",
       "           (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "           (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "           (relu): ReLU()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (layerNorm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (layerNorm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.1, inplace=False)\n",
       "         (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "       (1): EncoderLayer(\n",
       "         (multiheadattention): Multiheadattention(\n",
       "           (fcQ): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (fcK): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (fcV): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (fcOut): Linear(in_features=256, out_features=256, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (ffn): FFN(\n",
       "           (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "           (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "           (relu): ReLU()\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (layerNorm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (layerNorm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "         (dropout1): Dropout(p=0.1, inplace=False)\n",
       "         (dropout2): Dropout(p=0.1, inplace=False)\n",
       "       )\n",
       "     )\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (mlp): Sequential(\n",
       "     (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "     (1): Linear(in_features=64, out_features=16, bias=True)\n",
       "     (2): GELU()\n",
       "     (3): Linear(in_features=16, out_features=8, bias=True)\n",
       "   )\n",
       " ),\n",
       " defaultdict(list,\n",
       "             {'Train Loss': [3.708222029408611,\n",
       "               2.334329636828358,\n",
       "               1.9620700184567517,\n",
       "               1.6959442059832266,\n",
       "               1.0824143600178904,\n",
       "               0.5539805130892066,\n",
       "               0.41168063681914036,\n",
       "               0.3946918504646575,\n",
       "               0.38666596006587206,\n",
       "               0.3707857761250074,\n",
       "               0.358605196632712,\n",
       "               0.35263851249360467,\n",
       "               0.35066827944075446,\n",
       "               0.3490133837162261,\n",
       "               0.3454228809155316,\n",
       "               0.34174199329904353,\n",
       "               0.33841438298206405,\n",
       "               0.33685005062604806,\n",
       "               0.3370539086273467,\n",
       "               0.3346059629641681,\n",
       "               0.3303082346203793,\n",
       "               0.32420127453557046,\n",
       "               0.32195518047686117,\n",
       "               0.3199140191790592,\n",
       "               0.3177957828776295,\n",
       "               0.31114206183479126,\n",
       "               0.3039620250819689,\n",
       "               0.30049352080698505,\n",
       "               0.2980920432810764,\n",
       "               0.2986644001358534,\n",
       "               0.2919299696071214,\n",
       "               0.28049707172876337,\n",
       "               0.2788072487272589,\n",
       "               0.2771392629678506,\n",
       "               0.27689008214084276,\n",
       "               0.2681653425275567,\n",
       "               0.2629136829024767,\n",
       "               0.25538080213554354,\n",
       "               0.25654879677818115,\n",
       "               0.2581856198994762,\n",
       "               0.25035137467650304,\n",
       "               0.2458979100226406],\n",
       "              'Train Accuracy': [0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0],\n",
       "              'Valid Loss': [2.6440712555198913,\n",
       "               1.9240868767355657,\n",
       "               1.7618302539655357,\n",
       "               1.3331764982004835,\n",
       "               0.685522800418222,\n",
       "               0.4250426349366546,\n",
       "               0.39159469277995407,\n",
       "               0.3863049141920296,\n",
       "               0.3745458615813286,\n",
       "               0.3615802325260867,\n",
       "               0.3544577400016177,\n",
       "               0.35239705860994425,\n",
       "               0.35163886133272937,\n",
       "               0.3496918951629833,\n",
       "               0.34679678993619933,\n",
       "               0.3441055775827663,\n",
       "               0.3417198788967861,\n",
       "               0.3411487625662688,\n",
       "               0.3395765144733866,\n",
       "               0.3366292563213664,\n",
       "               0.33840304651078146,\n",
       "               0.3327225450497524,\n",
       "               0.33260982621247603,\n",
       "               0.3303360198713412,\n",
       "               0.3282006502531137,\n",
       "               0.3250592278826768,\n",
       "               0.32303788593620253,\n",
       "               0.3225363371478524,\n",
       "               0.32387557948470874,\n",
       "               0.3212295083483313,\n",
       "               0.3185038655806499,\n",
       "               0.32071771249649633,\n",
       "               0.32015845749028926,\n",
       "               0.3222711196370945,\n",
       "               0.323979553523337,\n",
       "               0.3277472534756752,\n",
       "               0.32672906700213245,\n",
       "               0.32507601133577385,\n",
       "               0.33080442354177975,\n",
       "               0.3262888005205021,\n",
       "               0.3280114288542681,\n",
       "               0.3282806338018672],\n",
       "              'Valid Accuracy': [0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0,\n",
       "               0.0]}))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_training(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=100, eta_min=1e-5),\n",
    "    device = device,\n",
    "    num_epochs = 2000,\n",
    "    metric_prefix=\"\",\n",
    "    file_prefix=\"\",\n",
    "    early_stopping=True,\n",
    "    early_stopping_step=10,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
