{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "import gc\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "HIDDEN_DIM = 256\n",
    "NUM_HEAD = 8 \n",
    "INNER_DIM = 512\n",
    "\n",
    "PAD_IDX = 0\n",
    "EOS_IDX = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddir = './'\n",
    "\n",
    "src_train_path = os.path.join(ddir,'src_train.pkl')\n",
    "src_valid_path = os.path.join(ddir,'src_valid.pkl')\n",
    "trg_train_path = os.path.join(ddir,'trg_train.pkl')\n",
    "trg_valid_path = os.path.join(ddir,'trg_valid.pkl')\n",
    "\n",
    "src_train_path2 = os.path.join(ddir,'src_train2.pkl')\n",
    "src_valid_path2 = os.path.join(ddir,'src_valid2.pkl')\n",
    "trg_train_path2 = os.path.join(ddir,'trg_train2.pkl')\n",
    "trg_valid_path2 = os.path.join(ddir,'trg_valid2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = joblib.load(src_train_path)\n",
    "src_valid = joblib.load(src_valid_path)\n",
    "trg_train = joblib.load(trg_train_path)\n",
    "trg_valid = joblib.load(trg_valid_path)\n",
    "\n",
    "src_train2 = joblib.load(src_train_path2)\n",
    "src_valid2 = joblib.load(src_valid_path2)\n",
    "trg_train2 = joblib.load(trg_train_path2)\n",
    "trg_valid2 = joblib.load(trg_valid_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(trg_train))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {}\n",
    "for i in range(len(labels)) :\n",
    "    labels_dict[labels[i]] = i\n",
    "labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiLabelEncoder(labels_dict, target) :\n",
    "    tmp = np.zeros((len(target), len(labels_dict)))\n",
    "    for t in range(len(target)) :\n",
    "        tmp[t][labels_dict[target[t]]] = 1\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_valid2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = multiLabelEncoder(labels_dict, trg_valid2)\n",
    "test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_train = multiLabelEncoder(labels_dict, trg_train)\n",
    "trg_valid = multiLabelEncoder(labels_dict, trg_valid)\n",
    "trg_train2 = multiLabelEncoder(labels_dict, trg_train2)\n",
    "trg_valid2 = multiLabelEncoder(labels_dict, trg_valid2)\n",
    "trg_valid2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 15*100*8\n",
    "SEQ_LEN = 60*2\n",
    "\n",
    "VOCAB_SIZE2 = 1108*8\n",
    "SEQ_LEN2 = 4674*2\n",
    "\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, src_data, trg_data):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_data = src_data\n",
    "        self.trg_data = trg_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "        \n",
    "    def __getitem__ (self, idx):\n",
    "        src = self.src_data[idx]\n",
    "        trg = self.trg_data[idx]\n",
    "\n",
    "        return torch.Tensor(src).long(), torch.Tensor(trg)\n",
    "\n",
    "train_dataset = TrainDataset(src_train2, trg_train2)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle= True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset(Dataset):\n",
    "    def __init__(self, src_data, trg_data):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_data = src_data\n",
    "        self.trg_data = trg_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "        \n",
    "    def __getitem__ (self, idx):\n",
    "        src = self.src_data[idx]\n",
    "        trg = self.trg_data[idx]\n",
    "\n",
    "        return torch.Tensor(src).long(), torch.Tensor(trg)\n",
    "\n",
    "valid_dataset = ValidDataset(src_valid2, trg_valid2)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle= False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__ (self, hidden_dim, inner_dim):\n",
    "        super().__init__()\n",
    " \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.inner_dim = inner_dim \n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, inner_dim)\n",
    "        self.fc2 = nn.Linear(inner_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "   \n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        output = self.fc1(output)\n",
    "        output2 = self.relu(output)\n",
    "        output2 = self.dropout(output)\n",
    "        output3 = self.fc2(output2)\n",
    "\n",
    "        return output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiheadattention(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, num_head: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # embedding_dim, d_model, 512 in paper\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # 8 in paper\n",
    "        self.num_head = num_head\n",
    "        # head_dim, d_key, d_query, d_value, 64 in paper (= 512 / 8)\n",
    "        self.head_dim = hidden_dim // num_head\n",
    "        self.scale = torch.sqrt(torch.FloatTensor()).to(device)\n",
    "\n",
    "        self.fcQ = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fcK = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fcV = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fcOut = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, srcQ, srcK, srcV, mask=None):\n",
    "\n",
    "        ##### SCALED DOT PRODUCT ATTENTION ######\n",
    "\n",
    "        Q = self.fcQ(srcQ)\n",
    "        K = self.fcK(srcK)\n",
    "        V = self.fcV(srcV)\n",
    "\n",
    "        Q = rearrange(\n",
    "            Q, 'bs seq_len (num_head head_dim) -> bs num_head seq_len head_dim', num_head=self.num_head)\n",
    "        K_T = rearrange(\n",
    "            K, 'bs seq_len (num_head head_dim) -> bs num_head head_dim seq_len', num_head=self.num_head)\n",
    "        V = rearrange(\n",
    "            V, 'bs seq_len (num_head head_dim) -> bs num_head seq_len head_dim', num_head=self.num_head)\n",
    "        \n",
    "        attention_energy = torch.matmul(Q, K_T)\n",
    "\n",
    "        if mask is not None :\n",
    " \n",
    "            attention_energy = torch.masked_fill(attention_energy, (mask == 0), -1e+4)\n",
    "            \n",
    "        attention_energy = torch.softmax(attention_energy, dim = -1)\n",
    "\n",
    "        result = torch.matmul(self.dropout(attention_energy),V)\n",
    "\n",
    "        ##### END OF SCALED DOT PRODUCT ATTENTION ######\n",
    "\n",
    "        # CONCAT\n",
    "        result = rearrange(result, 'bs num_head seq_len head_dim -> bs seq_len (num_head head_dim)')\n",
    "\n",
    "        result = self.fcOut(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_head, inner_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_head = num_head\n",
    "        self.inner_dim = inner_dim\n",
    "        \n",
    "        self.multiheadattention = Multiheadattention(hidden_dim, num_head)\n",
    "        self.ffn = FFN(hidden_dim, inner_dim)\n",
    "        self.layerNorm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layerNorm2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "\n",
    "\n",
    "    def forward(self, input, mask = None):\n",
    "\n",
    "        output = self.multiheadattention(srcQ= input, srcK = input, srcV = input, mask = mask)\n",
    "        output = self.dropout1(output)\n",
    "        output = input + output\n",
    "        output = self.layerNorm1(output)\n",
    "\n",
    "        output_ = self.ffn(output)\n",
    "        output_ = self.dropout2(output_)\n",
    "        output = output + output_\n",
    "        output = self.layerNorm2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__ (self, N, hidden_dim, num_head, inner_dim,max_length=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # N : number of encoder layer repeated \n",
    "        self.N = N\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_head = num_head\n",
    "        self.inner_dim = inner_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=VOCAB_SIZE, embedding_dim=hidden_dim, padding_idx=-1)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, num_head, inner_dim) for _ in range(N)])\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        batch_size = input.shape[0]\n",
    "        seq_len = input.shape[1]\n",
    "\n",
    "\n",
    "        pos = torch.arange(0, seq_len).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
    "\n",
    "        output = self.dropout(self.embedding(input) + self.pos_embedding(pos))\n",
    "\n",
    "        # Dropout\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        # N encoder layer\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, N = 2, hidden_dim = 256, num_head = 64, inner_dim = 512):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(N, hidden_dim, num_head, inner_dim)\n",
    "        self.mlp = nn.Sequential(nn.Linear(256*SEQ_LEN, 64),\n",
    "                                 nn.Linear(64,16),\n",
    "                                 nn.GELU(),\n",
    "                                 nn.Linear(16,8)\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        \n",
    "        # src = torch.flatten(src, start_dim=0)\n",
    "        \n",
    "        output = self.encoder(src)\n",
    "        output = torch.flatten(output, start_dim=1)\n",
    "        pred = self.mlp(output)\n",
    "    \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(src_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(N, HIDDEN_DIM, NUM_HEAD, INNER_DIM).to(device)\n",
    "model.eval()\n",
    "x = torch.Tensor(src_train[:10]).long().to(device)\n",
    "y = model(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 1e-4, weight_decay = 0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "    running_accuracy = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "\n",
    "    for step, (src, trg) in bar:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        pred = model(src=src)\n",
    "        \n",
    "        loss = criterion(F.softmax(pred, dim=0), trg)\n",
    "\n",
    "        loss.backward()\n",
    "    \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)  \n",
    "     \n",
    "        optimizer.step()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # change learning rate by Scheduler\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_accuracy = torch.mean((torch.argmax(pred, axis=1) == torch.argmax(trg, axis=1)).type(torch.FloatTensor))\n",
    "\n",
    "        accuracy += running_accuracy\n",
    "\n",
    "        dataset_size += batch_size\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        bar.set_postfix(\n",
    "            Epoch=epoch, Train_Loss=epoch_loss, LR=optimizer.param_groups[0][\"lr\"], accuracy=accuracy / np.float(\n",
    "                step+1)\n",
    "        )\n",
    "\n",
    "    accuracy /= len(dataloader)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "\n",
    "    for step, (src, trg) in bar:\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "\n",
    "        pred = model(src = src)\n",
    "        loss = criterion(F.softmax(pred, dim=0), trg)\n",
    "\n",
    "        running_loss += loss.item() * batch_size\n",
    "        dataset_size += batch_size\n",
    "\n",
    "     \n",
    "        val_loss = running_loss / dataset_size\n",
    "        running_accuracy = torch.mean((torch.argmax(pred, axis=1) == torch.argmax(trg, axis=1)).type(torch.FloatTensor))\n",
    "  \n",
    "        accuracy += running_accuracy\n",
    "\n",
    "        bar.set_postfix(\n",
    "            Epoch=epoch, Valid_Loss=val_loss, LR=optimizer.param_groups[0][\"lr\"], accuracy = accuracy / np.float(step + 1)\n",
    "        )\n",
    "\n",
    "    accuracy /= len(dataloader)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    metric_prefix=\"\",\n",
    "    file_prefix=\"\",\n",
    "    early_stopping=True,\n",
    "    early_stopping_step=10,\n",
    "):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU:{}\\n\".format(torch.cuda.get_device_name()))\n",
    "\n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        gc.collect()\n",
    "\n",
    "        train_epoch_loss, train_accuracy = train_one_epoch(\n",
    "            model,\n",
    "            optimizer,\n",
    "            scheduler,\n",
    "            dataloader= train_dataloader,\n",
    "            device=device,\n",
    "            epoch=epoch,\n",
    "        )\n",
    "\n",
    "        val_loss, val_accuracy = valid_one_epoch(\n",
    "            model, valid_dataloader, device=device, epoch=epoch\n",
    "        )\n",
    "\n",
    "        history[f\"{metric_prefix}Train Loss\"].append(train_epoch_loss)\n",
    "        history[f\"{metric_prefix}Train Accuracy\"].append(train_accuracy)\n",
    "        history[f\"{metric_prefix}Valid Loss\"].append(val_loss)\n",
    "        history[f\"{metric_prefix}Valid Accuracy\"].append(val_accuracy)\n",
    "\n",
    "\n",
    "        print(f\"Valid Loss : {val_loss}\")\n",
    "\n",
    "        if val_loss <= best_loss:\n",
    "            early_stop_counter = 0\n",
    "\n",
    "            print(\n",
    "                f\"Validation Loss improved( {best_loss} ---> {val_loss}  )\"\n",
    "            )\n",
    "\n",
    "            # Update Best Loss\n",
    "            best_loss = val_loss\n",
    "            \n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            PATH = \"{}epoch{:.0f}_Loss{:.4f}.bin\".format(file_prefix, epoch, best_loss)\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            torch.save(model.state_dict(), f\"{file_prefix}best_{epoch}epoch.bin\")\n",
    "\n",
    "            print(f\"Model Saved\")\n",
    "\n",
    "        elif early_stopping:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter > early_stopping_step:\n",
    "                break\n",
    "        \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print(\n",
    "        \"Training complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 3600,\n",
    "            (time_elapsed % 3600) // 60,\n",
    "            (time_elapsed % 3600) % 60,\n",
    "        )\n",
    "    )\n",
    "    print(\"Best Loss: {:.4f}\".format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_training(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=100, eta_min=1e-5),\n",
    "    device = device,\n",
    "    num_epochs = 20000,\n",
    "    metric_prefix=\"\",\n",
    "    file_prefix=\"\",\n",
    "    # early_stopping=True,\n",
    "    # early_stopping_step=10,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
