{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_list = ['이낙연', '이재명', '정세균', '이광재']\n",
    "start_day = 7 # 시작일은 7일 전\n",
    "end_day = 1 # 끝일은 하루 전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈과 라이브러리를 로딩\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time, os, math, random, sys, datetime\n",
    "from datetime import date, timedelta, datetime\n",
    "import pyautogui\n",
    "\n",
    "# chromedriver 자동 설치\n",
    "import chromedriver_autoinstaller\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "\n",
    "# 작업 시간과 고유 dir 등 생성\n",
    "n = time.localtime()\n",
    "s = '%04d-%02d-%02d' % (n.tm_year, n.tm_mon, n.tm_mday)\n",
    "s_time = time.time( )\n",
    "\n",
    "\n",
    "a = datetime.now()-timedelta(days=start_day)\n",
    "a = a.strftime('%Y.%m.%d')\n",
    "b = datetime.now()-timedelta(days=end_day)\n",
    "b = b.strftime('%Y.%m.%d')\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" *80)\n",
    "print(\"         Naver & DAUM NEWS 크롤러입니다.\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "f_dir = os.path.expanduser('~') + f'\\\\Desktop\\\\weekly\\\\{a}-{b}\\\\' # 운영체제 상관없이 바탕화면 'weekly' 폴더에 작업\n",
    "\n",
    "print(\"=\" *80)\n",
    "print(\"다음뉴스 크롤링을 시작합니다.\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "# 저장될 파일 경로와 이름을 지정\n",
    "# 파일을 저장할 폴더 위치를 만들고, 중복일 경우 해당 폴더를 삭제하고 새로 생성합니다.\n",
    "try : \n",
    "    os.makedirs(f_dir)\n",
    "except : pass\n",
    "else : \n",
    "    os.chdir(f_dir)\n",
    "\n",
    "# 크롤링 데이터가 들어갈 리스트 생성\n",
    "domain2 = []    # 크롤링 도메인\n",
    "keyword2 = []   # 크롤링 키워드\n",
    "title2 = []     # 게시물 제목\n",
    "user2 = []      # 게시물 작성자\n",
    "date2 = []      # 게시물 작성 시간\n",
    "\n",
    "\n",
    "# chromedriver-autoinstaller 이용\n",
    "chromedriver_autoinstaller.install(cwd=True)\n",
    "\n",
    "# HEADLESS MODE\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('headless')\n",
    "options.add_argument('window-size=1920x1080') \n",
    "# options.add_argument(\"--disable-gpu\")\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "args = [\"hide_console\", ]\n",
    "\n",
    "# 알림창 끄기\n",
    "options.add_experimental_option(\"prefs\", {\"profile.default_content_setting_values.notifications\": 1})\n",
    "\n",
    "# 웹사이트 접속 후 해당 메뉴로 이동\n",
    "driver = webdriver.Chrome(options=options,service_args=args)\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_range(start, end):\n",
    "    start = datetime.strptime(start, \"%Y.%m.%d\")\n",
    "    end = datetime.strptime(end, \"%Y.%m.%d\")\n",
    "    dates = [date.strftime(\"%Y.%m.%d\") for date in pd.date_range(start, periods=(end-start).days+1)]\n",
    "    return dates\n",
    "    \n",
    "dates = date_range(a, b)\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 다음 뉴스 이동\n",
    "\n",
    "\n",
    "for keyword in keyword_list :\n",
    "    print(\"keyword:\", keyword)\n",
    "\n",
    "    for date in dates :\n",
    "        \n",
    "        page_num = 1\n",
    "\n",
    "        while True :\n",
    "\n",
    "            query_url= (f'https://search.daum.net/search?&at=more&cluster=n&dc=STC&ed={str(date.replace(\".\",\"\"))}235959&https_on=on&p={page_num}&period=u&pg=1&q={keyword}&r=1&rc=1&sd={str(date.replace(\".\",\"\"))}000000&sort=recency&w=news')\n",
    "            driver.get(query_url)\n",
    "            driver.implicitly_wait(5)\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')        \n",
    "            try :\n",
    "                item_box = soup.find(name='ul', attrs={'id':'newsResultUL'}).find_all('li')  \n",
    "\n",
    "                if len(title2) > 0 :            \n",
    "                    if title2[-len(item_box)] == item_box[0].find(name='div', attrs={'class':'wrap_cont'}).find(name='div',attrs={'class':'wrap_tit mg_tit'}).text :\n",
    "                        print(f\"현재 페이지 {page_num+1}, 현재 키워드 : {keyword} 크롤링을 종료합니다.\")\n",
    "                        break\n",
    "\n",
    "                for item in item_box :\n",
    "\n",
    "                    if item.find(name='div', attrs={'class':'wrap_cont'}) :                \n",
    "\n",
    "                        domain1 = \"DaumNews\"\n",
    "                        keyword1 = keyword\n",
    "\n",
    "                        news = item.find(name='div', attrs={'class':'wrap_cont'})\n",
    "\n",
    "        #                 print(news)\n",
    "        #                 break\n",
    "\n",
    "                        user1 = news.find(name='span', attrs={'class':'f_nb date'}).text.split(\" | \")[1]\n",
    "                        date1 = date\n",
    "                        # date1 = news.find(name='span', attrs={'class':'f_nb date'}).text.split(\" | \")[0]\n",
    "\n",
    "                        title1 = item.find(name='div',attrs={'class':'wrap_tit mg_tit'}).text\n",
    "                        \n",
    "#                         url1 = \n",
    "\n",
    "                        print(f'keyword : {keyword1}')\n",
    "                        print(f'user : {user1}')\n",
    "                        print(f'date : {date1}')\n",
    "                        print(f'title : {title1}')\n",
    "\n",
    "                        print(\"\\n\")\n",
    "\n",
    "                        domain2.append(domain1)\n",
    "                        keyword2.append(keyword1)\n",
    "                        date2.append(date1)\n",
    "                        title2.append(title1)\n",
    "                        user2.append(user1)\n",
    "\n",
    "\n",
    "\n",
    "                page_num += 1\n",
    "            except :\n",
    "                break\n",
    "\n",
    "\n",
    "### Naver News\n",
    "\n",
    "\n",
    "# 작업 시간과 고유 dir 등 생성\n",
    "n = time.localtime()\n",
    "s = '%04d-%02d-%02d' % (n.tm_year, n.tm_mon, n.tm_mday)\n",
    "s_time = time.time( )\n",
    "\n",
    "\n",
    "a = datetime.now()-timedelta(days=end_day)\n",
    "a = a.strftime('%Y.%m.%d')\n",
    "b = datetime.now()-timedelta(days=start_day)\n",
    "b = b.strftime('%Y.%m.%d')\n",
    "print(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\" *80)\n",
    "\n",
    "f_dir = os.path.expanduser('~') + f'\\\\Desktop\\\\weekly\\\\{b}-{a}\\\\' # 운영체제 상관없이 바탕화면 'weekly' 폴더에 작업\n",
    "\n",
    "print(\"=\" *80)\n",
    "print(\"네이버뉴스 크롤링을 시작합니다.\")\n",
    "print(\"=\" *80)\n",
    "\n",
    "\n",
    "# 저장될 파일 경로와 이름을 지정\n",
    "# 파일을 저장할 폴더 위치를 만들고, 중복일 경우 해당 폴더를 삭제하고 새로 생성합니다.\n",
    "try : \n",
    "    os.makedirs(f_dir)\n",
    "except : pass\n",
    "else : \n",
    "    os.chdir(f_dir)\n",
    "\n",
    "fc_name=f_dir+'\\\\'+f'news.csv'\n",
    "\n",
    "\n",
    "\n",
    "# 1. 네이버 이동\n",
    "\n",
    "dates = date_range(b, a)\n",
    "\n",
    "for keyword in keyword_list :\n",
    "    print(\"keyword:\", keyword)\n",
    "\n",
    "    for date in dates :\n",
    "\n",
    "        page_num = 0\n",
    "\n",
    "\n",
    "        while True :\n",
    "\n",
    "            query_url= (f'https://search.naver.com/search.naver?where=news&sm=tab_pge&query={keyword}&sort=1&photo=0&field=0&pd=3&ds={date}&de={date}&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,p:from{b}to{a},a:all&start={page_num}1')\n",
    "            driver.get(query_url)\n",
    "            driver.implicitly_wait(5) \n",
    "            \n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')        \n",
    "            item_box = soup.find_all(name='li', attrs={'class':'bx'})\n",
    "            \n",
    "            if len(item_box) == 6 :\n",
    "                print(f\"현재 페이지 {page_num+1}, 현재 키워드 : {keyword} 크롤링을 종료합니다.\")\n",
    "                break\n",
    "\n",
    "            for item in item_box :\n",
    "                \n",
    "                if item.find(name='div', attrs={'class':'news_area'}) :\n",
    "                    \n",
    "\n",
    "                    domain1 = \"NaverNews\"\n",
    "                    keyword1 = keyword\n",
    "\n",
    "                    news = item.find(name='div', attrs={'class':'news_area'})\n",
    "                    \n",
    "    #                 print(news)\n",
    "    #                 break\n",
    "    \n",
    "                    user1 = news.find(name='div', attrs={'class':'news_info'}).find(name='a', attrs={'class':'info press'}).text\n",
    "\n",
    "                    date1 = date                    \n",
    "                    # try : \n",
    "                    #     date1 = news.find(name='div', attrs={'class':'news_info'}).find_all(name='span', attrs={'class':'info'})[-1].text\n",
    "                    # except :\n",
    "                    #     date1 = news.find(name='div', attrs={'class':'news_info'}).find(name='span', attrs={'class':'info'}).text\n",
    "                    title1 = item.find(name='a',attrs={'class':'news_tit'})['title']\n",
    "\n",
    "                    print(f'user : {user1}')\n",
    "                    print(f'date : {date1}')\n",
    "                    print(f'title : {title1}')\n",
    "\n",
    "                    print(\"\\n\")\n",
    "\n",
    "                    domain2.append(domain1)\n",
    "                    keyword2.append(keyword1)\n",
    "                    date2.append(date1)\n",
    "                    title2.append(title1)\n",
    "                    user2.append(user1)\n",
    "                \n",
    "                \n",
    "                    \n",
    "            page_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df['domain'] = pd.Series(domain2)\n",
    "df['keyword'] = pd.Series(keyword2)\n",
    "df['user_text'] = pd.Series(title2)\n",
    "df['channel'] = pd.Series(user2)\n",
    "df['date'] = pd.Series(date2)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 형태로 저장하기\n",
    "df.to_csv(fc_name,encoding=\"utf-8-sig\",index=False)\n",
    "\n",
    "e_time = time.time( )\n",
    "t_time = e_time - s_time\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"=\" *80)\n",
    "print(\"1.총 소요시간은 %s 초 입니다 \" %round(t_time,1))\n",
    "print(\"2.파일 저장 완료: csv 파일명 : %s \" %fc_name)\n",
    "\n",
    "driver.quit()\n",
    "sys.exit(\"프로그램을 종료합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
